% !TEX root = ../thesis-example.tex
%
\chapter{Natural Language Processing}
\label{sec:nlp}
In diesem Kapitel wird der theoretische Hintergrund, die Methoden des Natural-Language-Processings, eingeführt. Nach den Grundlagen werden zwei verschiedene Methoden zum Parsen vorgestellt. 

\section{Grundlagen}
\label{sec:nlp:grundlagen}

Betrachtet man einen Satz in einer natürlichen Sprache, die im Rahmen dieser Arbeit auf Englisch festgelegt ist, so kann ein Algorithmus dessen Semantik nicht ohne Weiteres herauslesen. Hierfür bedarf es verschiedener Hilfsstrukturen und zusätzlicher Informationen. Als ersten Schritt bietet es sich an, den einzelnen Wörtern eines Satzes ihre Wortart zuzuordnen. Mit Wortart, alternativ auch Wortklasse oder im Englischen part-of-speech (POS), ist gemeint, wie ein Wort im Satz auftritt. Beispiele für Wortarten sind Nomen, Verb und Adjektiv. In dieser Arbeit wird das Tagset, also die Menge an Wortarten, aus der Penn Treebank verwendet. Siehe hierfür Tabelle \ref{tab:pos-tags}.
\begin{table}
\begin{tabular}{ | l l l |}
	\hline
	\multicolumn{3}{|c|}{Penn Treebank Part-of-Speech Tags} \\
	\hline
	\hline
	Tag & Beschreibung & Beispiel \\
	\hline
	CC & Koordinierende Konjunktion & \textit{and} \\
	CD & Kardinalzahl & \textit{third}\\
	DT & Artikel & \textit{the}\\
	EX & Existentielles \textit{there} & \textit{there} is\\
	FW & Fremdword & \textit{les}\\
	IN & Präposition, unterordnende Konjunktion & \textit{in}\\
	JJ & Adjektiv & \textit{green}\\
	JJR & Adjektiv, Komparativ & \textit{greener}\\
	JJS & Adjektiv, Superlativ & \textit{greenest}\\
	LS & Listenelement Markierung & \textit{1)}\\
	MD & Modal & \textit{could}\\
	NN & Nomen, singular oder Masse & \textit{table}\\
	NNS & Nomen, plural & \textit{tables}\\
	NNP & Eingenname, singular & \textit{Germany}\\
	NNPS & Eigenname, plural & \textit{Vikings}\\
	PDT & Predeterminer & \textit{both} his children \\
	POS & possesive Endung & \textit{'s} \\
	PRP & Personalpronomen & \textit{me} \\
	PRP\$ & Possesivpronomen & \textit{my} \\
	RB & Adverb & \textit{extremely} \\
	RBR & Adverb, Komparativ & \textit{better} \\
	RBS & Adverb, Superlativ & \textit{best} \\
	RP & Partikel & \textit{about} \\
	SYM & Symbol & \textit{\%} \\
	TO & to & what \textit{to} do \\
	UH & Ausruf & \textit{oops} \\
	VB & Verb, Grundform & \textit{be} \\
	VBD & Verb, Vergangenheitsform & \textit{was} \\
	VBG & Verb, Gerund \textbackslash Partizip Präsens & \textit{being} \\
	VBN & Verb, Partizip Perfekt & \textit{been} \\
	VBP & Verb, Präsens, nicht 3.Person Singular & \textit{am}  \\
	VBZ & Verb, Präsens, 3.Person Singular & \textit{is} \\
	WDT & \textit{wh}-Artikel & \textit{which} \\
	WP & \textit{wh}-Pronomen & \textit{who} \\
	WP\$ & \textit{wh}-Possesivpronomen & \textit{whose} \\
	WRB & \textit{wh}-Adverb & \textit{be} \\
	\hline
\end{tabular}
\caption{Penn Treebank POS Tags} %TODO Ref paper oder https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html Stand 08.05. 12:00
\label{tab:pos-tags}
\end{table}
Der Satz
\begin{quote}
My dog also likes eating sausage.
\end{quote}
würde mit diesem Tagset also folgendermaßen annotiert werden:
\begin{quote}
My/PRP\$ dog/NN also/RB likes/VBZ eating/VBG sausage/NN ./.
\end{quote}
Wie ein Satz maschinell mit POS-Tags versehen werden kann, wird in dieser Arbeit nicht weiter behandelt. 
Über diese zusätzliche Notation hinaus kann man erkennen, dass sich in der englischen Sprache oftmals mehrere Wörter als Gruppe oder als eine Komponente innerhalb des Satzes verhalten. So eine Gruppe wäre zum Beispiel die Nominalphrase \textit{My Dog} oder die Verbalphrase \textit{likes eating sausage}. Auch hier wird wieder die Annotation der Penn Treebank verwendet, abgebildet in Tabelle \ref{tab:phrase-tags}. Zu diesem Satz an Tags gibt es noch die Erweiterung um relationale Tags. Diese liefern eine Zusatzinformation und werden an die eben vorgestellten angehängt. Zum Beispiel bekommt das Subjekt eines Satzes das Suffix \textit{-SBJ}. Das heißt aus \textit{NP} wird \textit{NP-SBJ}. Dieser Erweiterungssatz wird im Rahmen dieser Arbeit herausgelassen, aber der Vollständigkeit halber erwähnt.
\begin{table}
\begin{tabular}{ | l p{7cm} p{4cm} |}
	\hline
	\multicolumn{3}{|c|}{Penn Treebank Syntactic Tags} \\
	\hline
	\hline
	Tag & Beschreibung & Beispiel \\
	\hline
	S & einfacher deklarativer Satz & \textit{There we go.} \\
	SBAR & Satz beginnend mit unterordnender Konjunktion & feels \textit{like we have to move} \\
	SBARQ & Direkte Frage beginnend mit \textit{wh}-Wort oder \textit{wh}-Phrase & \textit{So what's that about?} \\
	SINV & Invertierter deklarativer Satz & neither \textit{am I a pessimist.} \\
	SQ & Invertierte Ja/Nein Frage oder Hauptsatz einer \textit{wh}-Frage & \textit{Will they move on?} \\
	
	ADJP & Adjektivphrase & \textit{relatively cheap} \\
	ADVP & Adverbphrase & \textit{down here} \\
	CONJP & Konjunktionalphrase &  \textit{but also for tissues}\\
	FRAG & Fragment & if \textit{not today}, ... \\
	INTJ & Zwischenruf &  \textit{Well} \\
	LST & Listenmarkierung & \textit{1} \\
	NAC & Keine Komponente & via the \textit{Freedom of Information} \\
	NP & Nominalphrase & \textit{the sun} \\
	NX & Markiert Kopf in komplexen NP & fresh \textit{apples and cinnamon} \\
	PP & Präpositionalphrase & \textit{in some way} \\
	PRN & Nebenläufige Phrase & \textit{..., bless his heart, ...} \\
	PRT & Partikel & \textit{up} \\
	QP & Quantifizierende Phrase & \textit{or two a day} \\
	RRC & Reduzierter Relativsatz & titles \textit{not presently in the collection} \\
	UCP & Ungleich Koordinierte Phrasen & She flew \textit{yesterday and on July 4th.} \\
	VP & Verbalphrase & this \textit{is my dog.} \\
	WHADJP	& \textit{Wh}-Adjektivphrase & \textit{how great you are} \\
	WHADVP & \textit{Wh}-Adverbphrase & \textit{When} I see it\\
	WHNP & \textit{Wh}-Nominalphrase & \textit{What} they've done \\
	WHPP & \textit{Wh}-Präpositional & \textit{At which} \\
	X & Unbekannt & \textit{The more} ..., \textit{the less} ... \\	
	
	\hline
\end{tabular}
\caption{Penn Treebank Syntaktische Tags} %TODO Ref paper Bracketing Guidelines for Treebank II Style 
\label{tab:phrase-tags}
\end{table}
In \ref{eqn:multiline-annotated-dog-eating} wird der Satz mit der syntaktischen Annotation dargestellt.

%TODO Def Bäume?
\begin{align}
&(S \nonumber \\ 
& \qquad (NP \;\;(PRP\$ \;\; My)\; (NN \;\; dog)) \nonumber \\
& \qquad (ADVP \;\;(RB \;\; also)) \nonumber \\
& \qquad (VP \;\;(VBZ \;\; likes) \nonumber \\
& \qquad \qquad (S \nonumber \\
& \qquad \qquad \qquad (VP \;\;(VBG eating) \nonumber \\
& \qquad \qquad \qquad (NP \;\;(NN \;\; sausage)))))\nonumber \\
& \qquad (. \;\; .))
\label{eqn:multiline-annotated-dog-eating}
\end{align}
Alternativ zur geklammerten Lösung kann man dieses Ergebnis auch als Syntaxbaum zeichnen, siehe hierfür Abbildung \ref{fig:syn-tree-dog-likes}. \\
\begin{figure}
\qtreecentertrue\Tree [.S [.NP [.PRP My ] [.NN dog ] ] [.ADVP [.RB also ] ] [.VP [.VBZ likes ] [.S [.VP [.VBG eating ] [.NP [.NN sausage ] ] ] ] ] [.. . ] ]
\caption{Syntaxbaum zum Satz \textit{My dog also likes eating sausage.}}
\label{fig:syn-tree-dog-likes}
\end{figure}
Wie man am Beispiel erkennen kann, sind, im Gegensatz zu den POS-Tags, auch diverse Verschachtelungen dieser Komponenten möglich. Um diese Anordnungsstruktur innerhalb einer Sprache zu beschreiben, bieten sich kontextfreie Grammatiken an. Diese Art der Grammatik zeichnet sich dadruch aus, dass sich auf der linken Seite der Regeln genau ein Nichtterminalsymbol und auf der rechten Seite können sich beliebig viele Terminale und Nichtterminale befinden. Das Nichtterminal auf der linken Seite hat keine Einschränkung welche Symbole sich um es herum befinden müssen. Es hat also keinen Kontext, daher die Bezeichnung kontextfrei. Für die Verarbeitung unseres Beispielsatzes wurden unter anderem folgende Regeln verwendet:
\begin{lstlisting}
S -> NP ADVP VP .
NP -> PRP$ NN
PRP$ -> My
NN -> dog
\end{lstlisting}
Da ein englischer Satz nicht unbedingt \textit{S} als oberstes Nichtterminal hat, sondern auch \textit{FRAG}, \textit{SBARQ} und andere möglich sind, muss in den Grammatiken ein zusätzliches Startsymbol eingeführt werden. Dieses wird zum Beispiel \textit{ROOT} oder \textit{TOP} genannt. %TODO Verweis auf Parser die das so machen oder noch mehr ausführen
Anhand des vollständigen Regelsatzes der Grammatik einer Sprache, kann man theoretisch jedem grammatikalisch korrekten Satz dieser Sprache seine syntaktische Struktur zuweisen. Es gibt für diverse natürliche Sprachen Sammlungen von annotierten Sätzen, diese werden Treebank oder Korpus genannt. Ein bekannter Korpus der englischen Sprache ist die Penn Treebank, aus welcher auch die hier verwendete Annotation stammt. Dieser Korpus wird vom Linguistic Data Consortium, mit Sitz in der Univertität von Pennsylvania, herausgegeben. %TODO https://www.ldc.upenn.edu/about#
%Er umfasst über 4,5 Millionen Wörter 
%TODO Recherche über PTB betreiben - aktuelle Infos bekommen - Seite 442 im Buch nochmal genauer anschauen
Im Rahmen des Penn Treebank Projekts wurden von 1989 bis 1992 über 4,5 Millionen Wörter der Treebank hinzugefügt. Die Texte hierfür stammen zu einem Großteil aus dem Wall Street Journal. Der Ursprung der Sätze in einer Treebank kann eine Rolle spielen, da Parser mit Hilfe von Treebanks trainiert werden. Dazu mehr in Kapitel \ref{sec:nlp:stat-parsen}. %TODO zitat von paper ptb_info

\section{Syntaktisches Parsen}
\label{sec:nlp:syn-parsen}

Syntaktisches Parsen wird als die "Aufgabe des Erkennens eines Satzes und des Zuweisens einer syntaktischen Struktur" definiert. %TODO zitat s.461
%TODO Def: Paren als richtige Def, danach gleich Def: Parser = Programm oder sowas, welches Parsen durchführt
Ein Parser ist damit ein Programm, das Sätze parst.
Zum Einstieg in das Kapitel wird das Parsen als Suche betrachtet. Das Suchproblem besteht darin, aus allen möglichen Bäumen, welche sich mit der Grammatik generieren lassen, den korrekten Baum zur Eingabe zu finden. Der Suchraum wird also von der Grammatik festgelegt. Ein Baum ist korrekt, wenn \textit{S} die Wurzel ist und exakt die Eingabe abgedeckt wird. Anhand dieser zwei Merkmale kann die Suche gestaltet werden. Somit ergeben sich als grundlegende Ansätze die Top-Down und die Bottom-Up Suche. \\
Beim Top-Down Verfahren wird mit dem Startsymbol \textit{S} begonnen und dieses mit den Regeln der Grammatik Schritt für Schritt erweitert. Bäume deren Blätter nicht auf die Eingabe passen werden abgelehnt. \\
Die Bottom-Up Suche beginnt, dem Namen entsprechend, am anderen Ende des Baumes. Im ersten Schritt gibt es nur die Eingabeworte als Blätter. Es wird mit den rechten Seiten der Grammatikregeln der Baum nach oben gebaut. Hier kann also ein Baum ausgeschlossen werden, wenn seine obersten Knoten in keiner Kombination auf keiner rechten Seite einer Produktion vorkommen. %Der Vorteil des einen Ansatz entspricht in etwa dem Nachteil des anderen. 
So werden mit der Top-Down Suche keine Bäume gebaut die niemals das Start Symbol als Wurzel haben, dafür wird aber die Eingabe im Allgemeinen nicht abgedeckt. Beim Bottom-Up Ansatz verhält es sich genau andersherum. \\
Das Hauptproblem, welches sich beim Finden des korrekten Baumes ergibt, ist die Mehrdeutigkeit. Zum einen können Wörter mehrdeutig sein, wie etwa das englische Wort \textit{book}, welches sowohl Verb als auch Nomen ist. Zum anderen, und für diesen Kontext relevanter, gibt es die strukturelle Mehrdeutigkeit. Ein Beispielsatz hierfür ist: 
\begin{quote}
I shot an elephant in my pajamas.
\end{quote}
In dieser Satzstruktur kann sich \textit{in my pajamas} sowohl auf den Erzähler, als auch auf den Elefanten beziehen und gibt es mehr als einen korrekten Baum. \\ % Grammatikalisch sind also mehrere Bäume korrekt, inhaltlich aber nur einer. %Hier besser ausdrücken
Diese Art der strukturellen Mehrdeutigkeit ist die Anhangs-Mehrdeutigkeit. Eine Komponente des Satzes, in diesem Fall die Präpostionalphrase, kann an mehreren Stellen angehängt werden. Kombiniert man die Präpositional- an die Verbalphrase hat der Satz die Bedeutung, dass das Schießen im Pyjama stattgefunden hat. Bindet man diese an das Nomen Elefant wird ausgesagt, dass dieser sich im Pyjama befindet. Grammatikalische Korrektheit ist in beiden Fällen gegeben. \\
Eine andere Art ist die Koordinations-Mehrdeutigkeit, welche in Verbindung mit Konjunktionen und Satzverbindungen auftritt. Beispielsweise kann der Satzausschnitt \textit{old men and women} unterschiedlich interpretiert werden. \textit{Old} kann sich auf \textit{men and women} oder nur auf \textit{men} beziehen. \\
Die Anzahl an unterschiedlichen und dennoch grammatikalisch korrekten Bäumen für einen Satz kann also groß sein. %TODO kann sogar exponentiell wachsen, Seite 467
Von diesen Bäumen beschreibt aber nur einer den Inhalt des Satzes so, wie er vom Autor gemeint ist. Ein Parser braucht also weitere Kriterien um sich zwischen den verschiedenen Möglichkeiten entscheiden zu können. Hierzu mehr in Kapitel \ref{sec:nlp:stat-parsen}. Ohne zusätzliche Informationen kann der Parser nur alle möglichen Bäume erstellen. Um das effizient zu bewerkstelligen bietet sich dynamisches Programmieren an. 

\subsection{Dynamische Programmierung}
\label{sec:nlp:syn-parsen:dyn-progr}

Dynamisches Programmieren ist hier sinnvoll, da beim Erstellen des Baumes im Allgemeinen Mehrfacharbeit anfällt. Baut der Parser zum Beispiel die Bäume von oben nach unten und versucht dabei die Eingabe von links nach rechts abzudecken, dann findet er in der Regel einen Baum der einen Teil des Satzes abdeckt. Da noch nicht die gesamte Eingabe enthalten ist, handelt es sich nicht um einen korrekten Baum. Dennoch kann dieser Baum als Teilbaum in tatsächlichen Lösung enthalten sein. Der Parser müsste ihn aber ohne dynamisches Programmieren immer wieder neu finden. Als Algorithmen zum Parsen haben sich das Chart Parsing, der Earley Algorithmus und der Cocke-Kasami-Younger Algorithmus etabliert.
%TODO komplettes Kapitel noch eintragen

\section{Statistisches Parsen}
\label{sec:nlp:stat-parsen}

In diesem Kapitel werden Mittel vorgestellt, welche dem Parser helfen sich zwischen mehreren, grammatikalisch korrekten Bäumen eines Eingabesatzes zu entscheiden. Hierfür benötigt er für jeden errechneten Baum zusätzlich die Wahrscheinlichkeit mit welcher dieser semantisch korrekt ist. Das heißt, jede Lösung ist mit einer Wahrscheinlichkeit versehen und es kann diejenige, dessen Wert am höchsten ist als Lösung gewählt werden. %TODO letzten Satz evtl raus nehmen und ausführlich bei PCFG erklären

\subsection{Probabilistische Kontextfreie Grammatiken}
\label{sec:nlp:stat-parsen:pcfg}

Die Anforderung, einem Baum eine Wahrscheinlichkeit zuzuweisen, lässt sich mit dem Konzept der probabilistischen kontextfreien Grammatiken (abgekürzt: PCFG) umsetzen. Abgesehen von zwei Erweiterungen haben die Produktionen einer PCFG die gleiche Form wie die der ursprünglichen kontextfreien Grammatik. Erstens wird jeder Regel eine Wahrscheinlichkeit \textit{p}, mit \( 0 \leq p \leq 1 \), hinzugefügt.
\begin{equation}
A \to \beta  [p]
\end{equation}
Diese gibt an, mit welcher Wahrscheinlichkeit die rechte Seite und der Vorbedingen der linken Seite auftritt.
\begin{equation}
p := P(A \to \beta | A)
\end{equation}
Es handelt sich also um eine bedingte Wahrscheinlichkeit.\\ 
Zweitens muss für jedes Nichtterminal die Summe der Wahrscheinlichkeiten aller seiner Produktionen 1 ergeben.
\begin{equation}
\sum_{\beta} P(A \to \beta) = 1
\end{equation}
Die Wahrscheinlichkeit für einen Baum errechnet sich dann durch das Produkt der Wahrscheinlichkeiten aller verwendeten Regeln. Eine Grammatik heißt konsistent, falls die Summe der Wahrscheinlichkeiten aller möglichen Sätze 1 ergibt. Inkosistenz tritt auf, falls eine Regel der Form \( A \to A \) gibt.\\
Mit dieser neuen Art von Grammatik ergeben sich auch neue Algorithmen zum Berechnen der Bäume. Sowohl der CKY als auch der Earley Algorithmus sind um den Faktor der Wahrscheinlichkeit erweiterbar, wobei der CKY Algorithmus mehr Verwendung findet.\\ %TODO diesen Satz heraus nehmen oder so lassen oder CKY erklären, dann aber auch normalen kurz erklären...
Einen Nutzen kann man aus der zugefügten Wahrscheinlichkeit nur dann ziehen, wenn ihr numerischer Wert Sinn ergibt. Um diesen Wert für jede Regel zu berechnen gibt es zwei Möglichkeiten. Zum einen kann dieser aus einer vollständig annotierten Treebank errechnet werden. Hierfür wird jedes Auftreten einer Regel und des entsprechenden Nichtterminals gezählt und dividiert: 
\begin{equation}
P(A \to \beta) = \frac{Anzahl(A \to \beta)}{ \sum_{\gamma} Anzahl(A \to \gamma)} = \frac{Anzahl(A \to \beta)}{Anzahl(A)}
\end{equation}
Falls man keine solche Treebank zur Verfügung hat, gibt es noch eine zweite Möglichkeit die Werte der PCFG festzulegen. Hierzu arbeitet der Parser einen unannotierter Textkorpus durch und versieht die Sätze mit Tags. Zu Beginn haben alle Produktionen eines Nichtterminals die selbe Wahrscheinlichkeit. Der Korpus wird iterativ durchlaufen und nach jedem Durchgang werden die Wahrscheinlichkeiten der Regeln angepasst. Das Anpassen passiert wie in der ersten vorgestellten Möglichkeit, da zu diesem Zeitpunkt eine annotierte Treebank vorhanden ist. Das Verfahren endet, wenn die Wahrscheinlichkeiten konvergieren. \\
Die resultierenden Werte in der Grammatik hängen also davon ab, mit welchem Korpus sie berechnet wurden. Hierbei spielen Größe und Textart eine große Rolle. Um beim Parsen eines Textes möglichst gute Ergebnisse zu erhalten, sollte der Parser eine PCFG verwenden, welche mit einem Text des selben Genres erstellt wurde. So kann man beispielsweise mit einer Treebank aus technischen Handbüchern, unabhängig von ihrer Größe, beim Parsen eines privaten Briefes keine guten Ergebnisse erwarten, da sich die Sprache zu sehr unterscheidet.\\ %TODO Paper suchen über auswirken der unterschiedlichen Treebank arten für Parser!! 'Die' Treebank gibt es nicht - Diese These be - oder widerlegen!
Außerdem weißt dieses neue Konzept auch zwei Nachteile auf.
Das erste Problem der PCFG ergibt sich aus der Kontextfreiheit. Die Wahrscheinlichkeit einer Produktion ist immer gleich, egal an welcher Stelle im Satz sie auftritt. In der tatsächlichen natürlichen Sprache ist das aber im Allgemeinen nicht der Fall. Beispielweise kann die gewählte Produktion einer Nominalphrase abhängig davon sein ob die Phrase Objekt oder Subjekt des Satzes ist. Da diese Information ohne weiteres nicht in der Grammatik nicht berücksichtigt werden kann muss der Mittelwert aus beiden Fällen gebildet werden. Dies führt dazu, dass entweder im Falle des Objekts oder des Subjekts häufig die falsche Regel angewendet wird. \\
Als zweite Schwachstelle ergibt sich, dass die einzelnen Wörter eine zu kleine Rolle spielen. Als Beispiel hierfür dient die bereits erklärte Anhangs-Mehrdeutigkeit aus Kapitel \ref{sec:nlp:syn-parsen}. Wiederum wird eine Präpositionalphrase betrachtet, welche entweder an eine Verbal- oder eine Nominalphrase angebunden wird. An welche von beiden hängt wieder allein von der Treebank ab. Dort kommt entweder die Produktion \[ VP  \to  \alpha \;\;  NP \;\; PP \] oder die Kombination aus \[ VP  \to  \alpha \;\; NP \] und \[ NP  \to  NP \;\; PP \] öfter vor. \( \alpha \) steht für eines der hier möglichen Verb-Nichtterminale, welches aber in beide Fällen immer das selbe ist und deswegen keine Rolle spielt. Wesentlich bessere Resultate sind hier erzielbar, wenn man das Verb aus \textit{VP}, das Nomen aus \textit{NP} und die Präposition aus \textit{PP} in die Entscheidungsfindung mit einbezieht. Es kann aus der Treebank die Information gewonnen werden, ob die gegebene Präposition sich öfter auf das Nomen oder das Verb bezieht. Angenommen es gibt eine Präposition die ausschließlich mit Verben in Verbindung steht. In der Treebank sind es nun aber die Nominalphrasen, an welche öfter Präpositionalphrasen gebunden werden. Dann wird die eben angenommene Präposition mit einer PCFG, welche mit der Treebank errechnet wurde, immer falsch zugeordnet. \\ Diese Schwäche der PCFG macht sich ebenso bei Koordinations-Mehrdeutigkeit bemerkbar. Beim Verbinden von Phrasen durch Konjunktionen kann wieder die konkrete Konjunktion und die Beziehung zu den entsprechenden Wörtern der zu verbindenden Phrasen betrachtet werden. Betrachtet man nochmal das Beispiel aus \ref{sec:nlp:syn-parsen}: \textit{old men and women}. Hier könnte eine Treebank die Information liefern, dass die Wörter \textit{men} und \textit{women} per \textit{and} öfter direkt miteinander verbunden werden, als dass nur eines von beiden das Adjektiv \textit{old} zugeordnet bekommt. \\
Eine mögliche Verbesserung der PCFG ergibt sich durch das Erweitern der Nichtterminale um die Information wessen Kind es ist. Es wird also ein \textit{NP} als \textit{NP\^{}S} geschrieben, wenn \textit{S} der Elternknoten ist oder als \textit{NP\^{}VP}, falls es \textit{VP} ist. Hierdurch ergeben sich zwei neue Regeln in der Grammatik und damit auch zwei neue Wahrscheinlichkeiten. Mit diesem Konzept kann dem Nichtterminal, obwohl die Kontextfreiheit im grammatikalischen Sinne nicht verletzt wird, ein Kontext gegeben werden. Allerdings wird, wenn jedes Nichtterminal für jeden möglichen Elternknoten eine neue Produktion erhält, die Grammatik enorm aufgeblasen. Nebeneffekt dieser neuen Größe ist, dass bei gleich bleibender Treebank für jede Regel weniger Trainingsdaten zur Verfügung stehen und damit die erhaltenen Wahrscheinlichkeitswerte ungenauer sind. Für einen gegebenen Korpus muss also ein Split-and-Merge Algorithmus ausgeführt werden, der errechnet wie weit eine Aufteilung der Nichtterminale sinnvoll ist.  

\subsection{Probabilistische Lexikalisierte Kontextfreie Grammatiken}
\label{sec:nlp:stat-parsen:plcfg}

Ein weiterer Ansatz, um bessere Parserergebnisse zu erhalten, ist das Lexikalisieren der Grammatik. Hierfür muss der Begriff des Kopfes, im Englischen Head, eingeführt werden. Die Idee ist, dass jede syntaktische Einheit einen Kopf, in Form eines Wortes aus dieser Einheit, besitzt. Es wird das Wort genommen, welches "im Satz am grammatikalisch wichtigsten ist". %TODO Def. einbauen und Seite 443 zitieren.
Da jedes Nichtterminal einer Syntaktischen Einheit oder einem POS-Tag entspricht, kann jedem Nichtterminal und jeder Produktion der Grammatik ein Kopf zugewiesen werden. Für das Finden des richtiges Wortes gibt es verschiedene Schritte. Begonnen wird indem jedes POS-Nichtterminal sein entsprechendes Kind als Head wählt. Dieses Wort wird dann im Baum nach oben weitergeben. Für jede syntaktische Einheit gibt es Regeln, anhand derer einer der Köpfe der Kinder ausgewählt und als eigener eingesetzt wird. Ein Beispiel ist der lexikalisierte Baum für den Satz /textit{workers dumped sacks into a bin}, dargestellt in Abbildung \ref{fig:lex-tree-dumped-sacks}. %TODO Ref Buch Seite 445
\\
\begin{figure}
\qtreecentertrue\Tree [.S(dumped) [.NP(workers) [.NNS(workers) workers ] ] [.VP(dumped) [.VDB(dumped) dumped ] [.NP(sacks) [.NNS(sacks) sacks ] ] [.PP(into) [.IN(into) into ] [.NP(bin) [.DT(a) a ] [.NN(bin) bin ] ] ] ] ]
\label{fig:lex-tree-dumped-sacks}
\caption{Lexikalisierter Baum, entnommen aus} %TODO zitieren
\end{figure}
Zusätzlich zum Kopfwort kann auch noch dessen POS-Tag abgespeichert werden. Somit wird \textit{S(dumped)} zu \textit{S(dumped, VBD)} und \textit{NNS(workers)} zu \textit{NNS(workers, NNS)}. Die Wahrscheinlichkeit der Regel 
\begin{equation}\label{eqn:lexikal-dumped-sacks}
VP(dumped, VBD)  \to  VBD(dumped, VBD) \;\;  NP(sacks, NNS) \;\; PP(into, IN) 
\end{equation} %TODO alle Formeln so bennen
ergibt sich wieder aus dem Inhalt der Treebank, nämlich durch die Formel
\begin{equation}
\frac{Anzahl(VP(dumped, VBD)  \to  VBD(dumped, VBD) \;  NP(sacks, NNS) \; PP(into, IN))}{Anzahl(VP(dumped, VBD))} 
\end{equation}
Dieser Wert ist 0, falls der Korpus keinen Satz mit \textit{dumped} \textit{sacks} \textit{into} enthält. Existiert kein Satz in welchem sich \textit{VP(dumped, VBD)} finden lässt, so ist dieser Wert nicht definiert. \\
Aufgrund dieses Problems bedarf es anderer Berechnungsvorschriften, wie zum Beispiel Collins Model 1. %TODO Collins Model 1 Paper zitieren
Es wird jede Produktion folgendermaßen betrachtet: \textit{H} ist der Kopf, \( L_i \) sind die Nichtterminale links und \( R_i \) die rechts davon. Alle Nichtterminale bleiben weiterhin lexikalisiert. Das Kopfwort wird mit \textit{h} bezeichnet. Damit ergibt sich die Form
\[ A \to L_n...L_1 H R_1...R_m \]
Zusätzlich wird an der Stelle \( L_{n+1} \) und \( R_{m+1} \) das Nichtterminal \textit{STOP} eingefügt um anzuzeigen, dass hiernach die Regel zu Ende ist. In drei Schritten wird die Wahrscheinlichkeit der Produktion errechnet:
\begin{enumerate}
\item Es wird der Kopf mit der Wahrscheinlichkeit \( P_H(H | A, h) \) generiert.
\item Alle Elemente rechts vom Kopf werden mit \( \displaystyle\prod_{i = 1}^{m+1} P_R(R_i | A, h, H) \), also einschließlich dem \textit{STOP}, generiert.
\item Alle Elemente links von \textit{H} werden mit \( \displaystyle\prod_{i = 1}^{n+1} P_L(L_i | A, h, H) \) generiert.
\end{enumerate}
Für die Regel \ref{eqn:lexikal-dumped-sacks} errechnet sich die Wahrscheinlichkeit über
\begin{align}
P & = P_H(VBD|VP, dumped) \nonumber \\ & \times P_R(NP(sacks, NNS)|VP, dumped, VBD) \nonumber \\ & \times P_R(PP(into, IN)|VP, dumped, VBD) \nonumber \\ & \times P_R(STOP|VP, dumped, VBD) \nonumber \\ & \times P_L(STOP|VP, dumped, VBD)
\end{align}
Im Gegensatz zu vorher muss also nicht die Kombination aus \textit{NP(sacks, NNS)}, \textit{PP(into, IN)} und \textit{VBD(dumped, VBD)} vorhanden sein. Es genügt, wenn jedes einzeln, als Kind von \textit{VP(dumped, VBD)} auf der entsprechenden Seite des Heads in der Treebank zu finden ist. %TODO eventuell erklären warum man das so machen kann: Markov, MLE, ... %TODO Distanz, Model 2 und 3 erklären?
Darüber hinaus wird das Modell noch um die Information der Distanz erweitert. Es wird also zusätzlich berücksichtigt wie weit ein Nichtterminal vom Kopf der Regel entfernt ist. 
Der Vollständigkeit halber ist außerdem zu erwähnen, dass es neben Model 1 noch zwei weitere Modelle gibt. Diese Modelle bilden die Grundlage für den Collins Parser. %TODO zu finden unter...