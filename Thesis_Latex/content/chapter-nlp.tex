% !TEX root = ../thesis-example.tex
%
\chapter{Natural Language Processing}
\label{sec:nlp}
In diesem Kapitel wird der theoretische Hintergrund und die Methoden des Natural-Language-Processings eingeführt. Nach den Grundlagen werden zwei verschiedene Methoden zum Parsen vorgestellt. 

\section{Grundlagen}
\label{sec:nlp:grundlagen}

Betrachtet man einen Satz in einer natürlichen Sprache, die im Rahmen dieser Arbeit auf Englisch festgelegt ist, so kann ein Algorithmus dessen Semantik nicht ohne Weiteres herauslesen. Hierfür bedarf es verschiedener Hilfsstrukturen und zusätzlicher Informationen. Als ersten Schritt bietet es sich an, den einzelnen Wörtern eines Satzes ihre Wortart zuzuordnen. Mit Wortart, alternativ auch Wortklasse oder im Englischen \textit{part-of-speech} (POS), ist gemeint, wie ein Wort im Satz auftritt. Beispiele für Wortarten sind Nomen, Verb und Adjektiv. In dieser Arbeit wird das \textit{Tagset}, also die Menge an Wortarten aus der \textit{Penn-Treebank}, verwendet. Siehe hierfür Tabelle \ref{tab:pos-tags}.

\begin{table}
\begin{tabular}{ | l l l |}
	\hline
	\multicolumn{3}{|c|}{Penn-Treebank Part-of-Speech Tags} \\
	\hline
	\hline
	Tag & Beschreibung & Beispiel \\
	\hline
	CC & Koordinierende Konjunktion & \textit{and} \\
	CD & Kardinalzahl & \textit{third}\\
	DT & Artikel & \textit{the}\\
	EX & Existentielles \textit{there} & \textit{there} is\\
	FW & Fremdword & \textit{les}\\
	IN & Präposition, unterordnende Konjunktion & \textit{in}\\
	JJ & Adjektiv & \textit{green}\\
	JJR & Adjektiv, Komparativ & \textit{greener}\\
	JJS & Adjektiv, Superlativ & \textit{greenest}\\
	LS & Listenelement Markierung & \textit{1)}\\
	MD & Modal & \textit{could}\\
	NN & Nomen, singular oder Masse & \textit{table}\\
	NNS & Nomen, plural & \textit{tables}\\
	NNP & Eingenname, singular & \textit{Germany}\\
	NNPS & Eigenname, plural & \textit{Vikings}\\
	PDT & Predeterminer & \textit{both} his children \\
	POS & possesive Endung & \textit{'s} \\
	PRP & Personalpronomen & \textit{me} \\
	PRP\$ & Possesivpronomen & \textit{my} \\
	RB & Adverb & \textit{extremely} \\
	RBR & Adverb, Komparativ & \textit{better} \\
	RBS & Adverb, Superlativ & \textit{best} \\
	RP & Partikel & \textit{about} \\
	SYM & Symbol & \textit{\%} \\
	TO & to & what \textit{to} do \\
	UH & Ausruf & \textit{oops} \\
	VB & Verb, Grundform & \textit{be} \\
	VBD & Verb, Vergangenheitsform & \textit{was} \\
	VBG & Verb, Gerund \textbackslash Partizip Präsens & \textit{being} \\
	VBN & Verb, Partizip Perfekt & \textit{been} \\
	VBP & Verb, Präsens, nicht 3.Person Singular & \textit{am}  \\
	VBZ & Verb, Präsens, 3.Person Singular & \textit{is} \\
	WDT & \textit{wh}-Artikel & \textit{which} \\
	WP & \textit{wh}-Pronomen & \textit{who} \\
	WP\$ & \textit{wh}-Possesivpronomen & \textit{whose} \\
	WRB & \textit{wh}-Adverb & \textit{be} \\
	\hline
\end{tabular}
\caption{Penn-Treebank POS Tags \cite{ptbInformationen}} %TODO Ref paper oder https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html Stand 08.05. 12:00
\label{tab:pos-tags}
\end{table}

Der Satz
\begin{quote}
My dog also likes eating sausage.
\end{quote}
würde mit diesem \textit{Tagset} also folgendermaßen annotiert werden:
\begin{quote}
My/PRP\$ dog/NN also/RB likes/VBZ eating/VBG sausage/NN ./.
\end{quote}
Wie ein Satz maschinell mit POS-Tags versehen werden kann, wird in dieser Arbeit nicht weiter behandelt. 
Über diese zusätzliche Notation hinaus kann man erkennen, dass sich in der englischen Sprache oftmals mehrere Wörter als Gruppe oder als eine Komponente innerhalb des Satzes verhalten. So eine Gruppe wäre zum Beispiel die Nominalphrase \textit{My Dog} oder die Verbalphrase \textit{likes eating sausage}. Auch hier wird wieder die Annotation der \textit{Penn-Treebank} verwendet, abgebildet in Tabelle \ref{tab:phrase-tags}. Zu diesem Satz an Tags gibt es noch die Erweiterung um relationale Tags. Diese liefern eine Zusatzinformation und werden an die eben vorgestellten angehängt. Zum Beispiel bekommt das Subjekt eines Satzes das Suffix \textit{-SBJ}. Das heißt aus einer Nominalphrase \textit{NP} wird, falls sie Subjekt ist \textit{NP-SBJ}. Dieser Erweiterungssatz wird im Rahmen dieser Arbeit herausgelassen, aber der Vollständigkeit halber erwähnt.
\begin{table}
\begin{tabular}{ | l p{7cm} p{4cm} |}
	\hline
	\multicolumn{3}{|c|}{Penn-Treebank Syntactic Tags} \\
	\hline
	\hline
	Tag & Beschreibung & Beispiel \\
	\hline
	S & einfacher deklarativer Satz & \textit{There we go.} \\
	SBAR & Satz beginnend mit unterordnender Konjunktion & feels \textit{like we have to move} \\
	SBARQ & Direkte Frage beginnend mit \textit{wh}-Wort oder \textit{wh}-Phrase & \textit{So what's that about?} \\
	SINV & Invertierter deklarativer Satz & neither \textit{am I a pessimist.} \\
	SQ & Invertierte Ja/Nein Frage oder Hauptsatz einer \textit{wh}-Frage & \textit{Will they move on?} \\
	
	ADJP & Adjektivphrase & \textit{relatively cheap} \\
	ADVP & Adverbphrase & \textit{down here} \\
	CONJP & Konjunktionalphrase &  \textit{but also for tissues}\\
	FRAG & Fragment & if \textit{not today}, ... \\
	INTJ & Zwischenruf &  \textit{Well} \\
	LST & Listenmarkierung & \textit{1} \\
	NAC & Keine Komponente & via the \textit{Freedom of Information} \\
	NP & Nominalphrase & \textit{the sun} \\
	NX & Markiert Kopf in komplexen NP & fresh \textit{apples and cinnamon} \\
	PP & Präpositionalphrase & \textit{in some way} \\
	PRN & Nebenläufige Phrase & \textit{..., bless his heart, ...} \\
	PRT & Partikel & \textit{up} \\
	QP & Quantifizierende Phrase & \textit{or two a day} \\
	RRC & Reduzierter Relativsatz & titles \textit{not presently in the collection} \\
	UCP & Ungleich Koordinierte Phrasen & She flew \textit{yesterday and on July 4th.} \\
	VP & Verbalphrase & this \textit{is my dog.} \\
	WHADJP	& \textit{Wh}-Adjektivphrase & \textit{how great you are} \\
	WHADVP & \textit{Wh}-Adverbphrase & \textit{When} I see it\\
	WHNP & \textit{Wh}-Nominalphrase & \textit{What} they've done \\
	WHPP & \textit{Wh}-Präpositional & \textit{At which} \\
	X & Unbekannt & \textit{The more} ..., \textit{the less} ... \\	
	
	\hline
\end{tabular}
\caption{Penn-Treebank syntaktische Tags \cite{penntagsetAusfuehrlich}} %TODO Ref paper Bracketing Guidelines for Treebank II Style 
\label{tab:phrase-tags}
\end{table}
In Abbildung \ref{fig:multiline-annotated-dog-eating} wird der Satz mit der syntaktischen Annotation dargestellt.

%TODO Def Bäume?
\begin{figure}
\begin{align}
&(S \nonumber \\ 
& \qquad (NP \;\;(PRP\$ \;\; My)\; (NN \;\; dog)) \nonumber \\
& \qquad (ADVP \;\;(RB \;\; also)) \nonumber \\
& \qquad (VP \;\;(VBZ \;\; likes) \nonumber \\
& \qquad \qquad (S \nonumber \\
& \qquad \qquad \qquad (VP \;\;(VBG eating) \nonumber \\
& \qquad \qquad \qquad (NP \;\;(NN \;\; sausage)))))\nonumber \\
& \qquad (. \;\; .)) \nonumber
\end{align}
\label{fig:multiline-annotated-dog-eating}
\caption{Syntaktische Annotation in mehrzeiliger, geklammerter Darstellung}
\end{figure}

Alternativ zur geklammerten Lösung kann man dieses Ergebnis auch als Syntaxbaum zeichnen, siehe hierfür Abbildung \ref{fig:syn-tree-dog-likes}. \\
\begin{figure}
\qtreecentertrue\Tree [.S [.NP [.PRP My ] [.NN dog ] ] [.ADVP [.RB also ] ] [.VP [.VBZ likes ] [.S [.VP [.VBG eating ] [.NP [.NN sausage ] ] ] ] ] [.. . ] ]
\caption{Syntaxbaum zu \textit{My dog also likes eating sausage.}}
\label{fig:syn-tree-dog-likes}
\end{figure}
Wie man am Beispiel erkennen kann, sind, im Gegensatz zu den POS-Tags, auch diverse Verschachtelungen dieser Komponenten möglich. Um diese Anordnungsstruktur innerhalb einer Sprache zu beschreiben, bieten sich kontextfreie Grammatiken an. Diese Art der Grammatik zeichnet sich dadurch aus, dass sich auf der linken Seite der Regeln genau ein Nichtterminalsymbol und auf der rechten Seite sich beliebig viele Terminale und Nichtterminale befinden können. Das Nichtterminal auf der linken Seite hat keine Einschränkung, welche Symbole sich um es herum befinden müssen. Es hat also keinen Kontext, daher die Bezeichnung kontextfrei. Für die Verarbeitung unseres Beispielsatzes wurden unter anderem folgende Regeln verwendet:
\begin{lstlisting}
S -> NP ADVP VP .
NP -> PRP$ NN
PRP$ -> My
NN -> dog
\end{lstlisting}
Da ein englischer Satz nicht unbedingt \textit{S} als oberstes Nichtterminal hat, sondern auch \textit{FRAG}, \textit{SBARQ} und andere möglich sind, muss in den Grammatiken ein zusätzliches Startsymbol eingeführt werden. Dieses wird zum Beispiel \textit{ROOT} oder \textit{TOP} genannt. \\
Anhand des vollständigen Regelsatzes der Grammatik einer Sprache, kann man theoretisch jedem grammatikalisch korrekten Satz dieser Sprache seine syntaktische Struktur zuweisen. Es gibt für diverse natürliche Sprachen Sammlungen von annotierten Sätzen. Diese werden \textit{Treebank} oder Korpus genannt. Aus den dort gespeicherten syntaktischen Strukturen kann man unter anderem eine Grammatik für die Sprache ableiten. Hierfür muss man alle verwendeten Verschachtelungen als Regel auffassen. \\
Ein bekannter Korpus der englischen Sprache ist die \textit{Penn-Treebank}, aus welcher auch die hier verwendete Annotation stammt. Dieser Korpus wird vom Linguistic Data Consortium, mit Sitz in der Univertität von Pennsylvania, herausgegeben. \cite{ldc} \\ %TODO https://www.ldc.upenn.edu/about#
%Er umfasst über 4,5 Millionen Wörter 
%TODO Recherche über PTB betreiben - aktuelle Infos bekommen - Seite 442 im Buch nochmal genauer anschauen
Im Rahmen des \textit{Penn-Treebank-Projekts} wurden von 1989 bis 1992 über 4,5 Millionen Wörter der \textit{Treebank} hinzugefügt. Die Texte hierfür stammen zu einem Großteil aus dem Wall Street Journal. Woher die Sätze einer \textit{Treebank} stammen spielt eine große Rolle, da Pars er mit Hilfe von \textit{Treebanks} trainiert werden. Dazu mehr in Kapitel \ref{sec:nlp:stat-parsen}. \cite{ptbInformationen} %TODO zitat von paper ptb_info

\section{Syntaktisches Parsen}
\label{sec:nlp:syn-parsen}

Syntaktisches Parsen wird als die ``Aufgabe des Erkennens eines Satzes und des Zuweisens einer syntaktischen Struktur'' definiert. %TODO zitat s.461
%TODO Def: Paren als richtige Def, danach gleich Def: Parser = Programm oder sowas, welches Parsen durchführt
Ein Parser ist damit ein Programm, das Sätze parst.\\
Zum Einstieg in das Kapitel wird das Parsen als Suche betrachtet. Das Suchproblem besteht darin, aus allen möglichen Bäumen, welche sich mit der Grammatik generieren lassen, den korrekten Baum zur Eingabe zu finden. Der Suchraum wird also von der Grammatik festgelegt. Ein Baum ist korrekt, wenn das Startsymbol die Wurzel ist und exakt die Eingabe abgedeckt wird. Vereinfachend wird angenommen, dass alle Sätze als oberstes Nichtterminal \textit{S} haben. Deshalb wird dieses als Startsymbol betrachtet. Anhand der zwei Merkmale eines korrekten Baumes kann die Suche gestaltet werden. Es ergeben sich als grundlegende Ansätze die Top-Down und die Bottom-Up Suche. \\
Beim Top-Down Verfahren wird mit dem Startsymbol \textit{S} begonnen und dieses mit den Regeln der Grammatik Schritt für Schritt erweitert. Bäume deren Blätter nicht auf die Eingabe passen werden abgelehnt. \\
Die Bottom-Up Suche beginnt, dem Namen entsprechend, am anderen Ende des Baumes. Im ersten Schritt gibt es nur die Eingabeworte als Blätter. Es wird mit den rechten Seiten der Grammatikregeln der Baum nach oben gebaut. Hier kann also ein Baum ausgeschlossen werden, wenn seine obersten Knoten in keiner Kombination auf keiner rechten Seite einer Produktion vorkommen. \\
So werden mit der Top-Down Suche keine Bäume gebaut die niemals das Startsymbol als Wurzel haben. Dafür entsprechen die Blätter nicht exakt der Eingabe. Beim Bottom-Up Ansatz verhält es sich genau andersherum.
\subsection{Ambiguität}

Das Hauptproblem, welches sich beim Finden des korrekten Baumes ergibt, ist die Mehrdeutigkeit oder auch genannt \textit{Ambiguität}. Zum einen können Wörter mehrdeutig sein, wie etwa das englische Wort \textit{book}, welches sowohl Verb als auch Nomen sein kann. Zum anderen, und für diesen Kontext relevanter, gibt es die strukturelle Mehrdeutigkeit. Ein Beispielsatz hierfür ist: 
\begin{quote}
I shot an elephant in my pajamas.
\end{quote}
In dieser Satzstruktur kann sich \textit{in my pajamas} sowohl auf den Erzähler, als auch auf den Elefanten beziehen und somit gibt es mehr als einen korrekten Baum. \\ 
Diese Art der strukturellen Mehrdeutigkeit ist die Anhangs-Mehrdeutigkeit, bzw. im Englischen \textit{Attachment Ambiguity}. Eine Komponente des Satzes, in diesem Fall die Präpositionalphrase, kann an mehreren Stellen angehängt werden. Kombiniert man die Präpositional- an die Verbalphrase, hat der Satz die Bedeutung, dass der Schütze einen Pyjama trägt. Bindet man diese an das Nomen ``Elefant'' wird ausgesagt, dass dieser sich im Pyjama befindet. Grammatikalische Korrektheit ist in beiden Fällen gegeben. \\
Eine weitere Art ist die Koordinations-Mehrdeutigkeit, im Englischen \textit{Coordination Ambiguity}, welche in Verbindung mit Konjunktionen und Satzverbindungen auftritt. Beispielsweise kann der Satzausschnitt \textit{old men and women} unterschiedlich interpretiert werden. \textit{Old} kann sich auf \textit{men and women} oder nur auf \textit{men} beziehen. \\
Bringt man beide Formen der Mehrdeutigkeit in einen Satz, wie z.B. 
\begin{quote}
I shot wild elephants and lions in my pajamas.
\end{quote}
ergeben sich folgende vier Möglichkeiten (die Klammerung stellt die Zusammengehörigkeit dar): %TODO kann sogar exponentiell wachsen, Seite 467
\begin{quote}
I shot [wild elephants] and [lions] in my pajamas.\\
I shot [wild elephants and lions] in my pajamas.\\
I shot [wild elephants] and [lions in my pajamas].\\
I shot [[wild elephants and lions] in my pajamas].\\
\end{quote}
In den ersten beiden Versionen bezieht sich \textit{in my pajamas} auf \textit{shot}, danach auf \textit{lions}, bzw. \textit{wild elephants and lions}. Auch \textit{wild} wechselt zwischen einem und zwei Bezugswörtern. Die Anzahl kann also exponentiell wachsen. %TODO ref s.467
Von diesen Interpretationen beschreibt nur eine den Inhalt des Satzes so, wie er vom Autor gemeint ist. Ein Parser braucht also weitere Kriterien, um sich zwischen den verschiedenen Optionen entscheiden zu können. Hierzu mehr in Kapitel \ref{sec:nlp:stat-parsen}. Ohne zusätzliche Informationen kann der Parser lediglich alle grammatikalisch möglichen Bäume erstellen. Um diese Aufgabe effizient zu bewerkstelligen bietet sich dynamisches Programmieren an. 

\subsection{Dynamische Programmierung}
\label{sec:nlp:syn-parsen:dyn-progr}

Dynamisches Programmieren ist hier sinnvoll, da beim Erstellen des Baumes im Allgemeinen Mehrfacharbeit anfällt. Baut der Parser zum Beispiel die Bäume von oben nach unten und versucht dabei die Eingabe von links nach rechts abzudecken, dann findet er in der Regel einen Baum der einen Teil des Satzes abdeckt. Da noch nicht die gesamte Eingabe enthalten ist, handelt es sich nicht um einen korrekten Baum. Dennoch wird dieser Baum in der Regel immer wieder gebaut, bzw. ist sogar in der tatsächlichen Lösung enthalten. Dass dieser Teilbaum nicht jedes mal neu gefunden werden muss, bietet es sich an ihn abzuspeichern. Als Algorithmen für das Parsen haben sich das \textit{Chart Parsing}, der \textit{Earley}- und der \textit{Cocke-Kasami-Younger}-Algorithmus (kurz \textit{CKY}) etabliert. Letzterer wird nachfolgend etwas genauer ausgeführt. 
%TODO komplettes Kapitel noch eintragen

\subsection{Cocke-Kasami-Younger Algorithmus}
\label{sec:nlp:syn-parsen:cky}

Zu Beginn des Algorithmus muss die verwendete Grammatik in die \textit{Chomsky-Normalform} (kurz \textit{CNF}) überführt werden. Die \textit{CNF} zeichnet sich dadurch aus, dass die rechte Seite aus genau zwei Nichtterminalen oder einem Terminal besteht. Das Überführen einer kontextfreien Grammatik in diese Form erfolgt nach einem einfachen Regelsatz und wird hier nicht näher betrachtet. Auch die Rückführung von der \textit{CNF} in die ursprüngliche Grammatik ist, mit zusätzlich abgespeicherten Informationen, möglich. Diese Rückführung wird gemacht, da die Ergebnisbäume der \textit{CNF} Struktur und nicht der eigentlichen Grammatik entsprechen. Somit ist auch eine semantische Analyse komplizierter. \\
Nach der Überführung in die \textit{CNF} wird für einen Satz mit \textit{n} Wörtern eine obere Dreiecksmatrix der Größe \((n+1) \times (n+1) \) aufgestellt. Die Positionen im Satz werden mit 0 beginnend durchnummeriert. Vor dem ersten Wort steht Index 0, nach dem letzten Index \textit{n}. Die Zelle \([i, j]\) enthält diejenigen Nichtterminale, welche die Eingabe von \textit{i} bis \textit{j} abdecken. Dabei gilt \( 0 \leq i < n-1 \), \( 1 \leq j < n \) und \(i < j\). Die Zelle \([0, n]\) gibt somit an, welche Nichtterminale die komplette Eingabe abdecken.\\
Die Zellen \([i, i+1]\) bilden die Diagonale der Matrix. Hier wird die Eigenschaft der \textit{CNF} ausgenützt, dass alle Terminale auf der rechten Seite der Produktionen alleine stehen und somit jedes von mindestens einem Nichtterminal abgedeckt wird.\\
Die Eigenschaft, dass Nichtterminale auf der rechten Seite immer zu zweit auftreten führt dazu, dass für die eine Zelle \([i, j]\) folgendes gilt. Für jede Regel \( A \to B\;C \) die \textit{i} bis \textit{j} abdeckt, gibt es ein \textit{k}, mit \( i < k < j\), sodass das \textit{B} den Abschnitt \([i, k]\) und \textit{C} \([k, j]\) abdeckt. Ersteres befindet sich in der selben Zeile, links von der entsprechenden Zelle. Letzteres ist in der selben Spalte, unterhalb zu finden. Das Füllen der Matrix muss von links nach rechts und von unten nach oben geschehen.\\
Eine gefüllte Matrix für den Satz \textit{Book the flight through Houston.} ist in Abbildung \ref{fig:cky-beispiel} dargestellt. Hierfür wurde eine sehr kleine Grammatik verwendet die nur knapp mehr als die verwendeten Wörter abdeckt. Diese ist entnommen aus %TODO seite ... ref seite  
, von wo auch die Abbildung entnommen ist. Neben der \textit{CNF} Regeln sind auch die ursprünglichen dargestellt.

\begin{figure} [h]
\includegraphics[width=\textwidth]{gfx/cky-beispiel.png} 
\label{fig:cky-beispiel}	
\caption{Beispielmatrix für den \textit{CKY}-Algorithmus, entnommen aus .. %TODO 
}	
\end{figure}

\begin{figure} [h]
\includegraphics[width=\textwidth]{gfx/grammatik-cky-beispiel.png} 
\label{fig:cky-beispiel}	
\caption{Grammatik für  entnommen aus .. %TODO 
}	
\end{figure}

Ohne zusätzliche Informationen gibt die Tabelle nur an, ob ein Satz grammatikalisch korrekt ist. Dies ist er, falls in Zelle \([0, n]\) das Startsymbol steht. Um außerdem die verwendeten Regeln, also den Baum, bzw. die Bäume zur Eingabe zu erhalten, muss der Algorithmus erweitert werden. Die Erweiterung besteht darin, zu jedem Nichtterminal in jeder Zelle zusätzlich zu speichern, aus welchen zwei anderen Nichtterminalen es entstanden ist. Darüber hinaus darf das gleiche Nichtterminal mehrmals in einer Zelle vorkommen, da es aus unterschiedlichen Nichtterminalen entstanden sein kann. Ein Beispiel zur Tabelle in Abbildung \ref{fig:cky-beispiel} ist in Zelle \([0, 5]\) das Symbol \textit{S}. Es gibt drei Möglichkeiten dieses zu erzeugen: 
\begin{align}
& [0, 1]:Verb \; und\; [1, 5]:NP \nonumber \\ & [0, 3]:VP \; und \; [3, 5]:PP \nonumber \\ & [0, 3]:X2 \; und \; [3, 5]:PP
\end{align}
Damit ergeben sich für die Eingabe drei verschiedene Syntaxbäume. Diese werden erstellt, indem man die Tabelle, beginnend mit dem jeweiligen Startsymbol, rückwärts durchläuft.

\section{Statistisches Parsen}
\label{sec:nlp:stat-parsen}

In diesem Abschnitt werden Mittel vorgestellt, welche dem Parser helfen sich zwischen mehreren, grammatikalisch korrekten Bäumen eines Eingabesatzes zu entscheiden. Hierfür benötigt er für jeden errechneten Baum zusätzlich die Wahrscheinlichkeit, mit welcher dieser semantisch korrekt ist. Das heißt, jede Lösung ist mit einer Wahrscheinlichkeit versehen und es kann diejenige, dessen Wert am höchsten ist als Lösung gewählt werden. %TODO letzten Satz evtl raus nehmen und ausführlich bei PCFG erklären

\subsection{Probabilistische Kontextfreie Grammatiken}
\label{sec:nlp:stat-parsen:pcfg}

Die Anforderung, einem Baum eine Wahrscheinlichkeit zuzuweisen, lässt sich mit dem Konzept der probabilistischen kontextfreien Grammatiken (kurz \textit{PCFG}) umsetzen. Abgesehen von zwei Erweiterungen haben die Produktionen einer \textit{PCFG} die gleiche Form wie die der ursprünglichen kontextfreien Grammatik. Erstens wird jeder Regel eine Wahrscheinlichkeit \textit{p}, mit \( 0 \leq p \leq 1 \), hinzugefügt.
\begin{equation}
A \to \beta  [p]
\end{equation}
Diese gibt an, mit welcher Wahrscheinlichkeit die rechte Seite und der Vorbedingung der linken Seite auftritt.
\begin{equation}
p := P(A \to \beta | A)
\end{equation}
Es handelt sich also um eine bedingte Wahrscheinlichkeit.\\ 
Zweitens muss für jedes Nichtterminal die Summe der Wahrscheinlichkeiten aller seiner Produktionen eins ergeben.
\begin{equation}
\sum_{\beta} P(A \to \beta) = 1
\end{equation}
Die Wahrscheinlichkeit für einen Baum errechnet sich dann durch das Produkt der Wahrscheinlichkeiten aller verwendeten Regeln. Eine Grammatik heißt konsistent, falls die Summe der Wahrscheinlichkeiten aller möglichen Sätze eins ergibt. Inkosistenz tritt auf, falls es eine Regel der Form \( A \to A \) gibt.\\
Mit dieser neuen Art von Grammatik ergeben sich auch neue Algorithmen zum Berechnen der Bäume. Sowohl der \textit{CKY}- als auch der \textit{Earley}-Algorithmus sind um den Faktor der Wahrscheinlichkeit erweiterbar, wobei der \textit{CKY}-Algorithmus mehr Verwendung findet.\\
Der in Kapitel \ref{sec:nlp:syn-parsen:cky} vorgestellte \textit{CKY}-Algorithmus wird dahingehend erweitert, dass in jeder Zelle für jedes Nichtterminal die Wahrscheinlichkeit gespeichert wird. Damit erhält man eine dreidimensionale \( (n+1) \times (n+1) \times V\) Matrix, wobei \textit{V} die Anzahl an Nichtterminalen in der Grammatik, und \textit{n} wie bisher die Anzahl an Wörtern im Satz, ist. \\
%TODO diesen Satz heraus nehmen oder so lassen oder CKY erklären, dann aber auch normalen kurz erklären...
Einen Nutzen kann man aus der zugefügten Wahrscheinlichkeit nur dann ziehen, wenn ihr numerischer Wert Sinn ergibt. Um diesen Wert für jede Regel zu berechnen, gibt es zwei Möglichkeiten. Zum einen kann dieser aus einer vollständig annotierten \textit{Treebank} errechnet werden. Hierfür wird jedes Auftreten einer Regel und des entsprechenden Nichtterminals gezählt und dividiert: 
\begin{equation}
P(A \to \beta) = \frac{Anzahl(A \to \beta)}{ \sum_{\gamma} Anzahl(A \to \gamma)} = \frac{Anzahl(A \to \beta)}{Anzahl(A)}
\end{equation}
Die Grammatik wird also anhand der annotierten Sätze der \textit{Treebank} trainiert. Die verwendeten Sätze werden Trainingsdaten genannt. \\
Falls man keine solche \textit{Treebank} zur Verfügung hat, gibt es noch eine zweite Möglichkeit die Werte der \textit{PCFG} festzulegen. Hierzu arbeitet der Parser einen unannotierter Textkorpus durch und versieht die Sätze mit Tags. Zu Beginn haben alle Produktionen eines Nichtterminals die selbe Wahrscheinlichkeit. Der Korpus wird iterativ durchlaufen und nach jedem Durchgang werden die Wahrscheinlichkeiten der Regeln angepasst. Das Anpassen passiert wie in der ersten vorgestellten Möglichkeit, da zu diesem Zeitpunkt eine annotierte \textit{Treebank} vorhanden ist. Das Verfahren endet, wenn die Wahrscheinlichkeiten konvergieren. \\
Die resultierenden Werte in der Grammatik hängen also davon ab, mit welchem Korpus sie berechnet wurden. Hierbei spielen Größe und Textart eine große Rolle. Um beim Parsen eines Textes möglichst gute Ergebnisse zu erhalten, sollte der Parser eine \textit{PCFG} verwenden, welche mit einem Text des selben Genres erstellt wurde. So kann man beispielsweise mit einer \textit{Treebank} aus technischen Handbüchern, unabhängig von ihrer Größe, beim Parsen eines privaten Briefes keine guten Ergebnisse erwarten, da sich die Sprache zu sehr unterscheidet.\\ %TODO Paper suchen über auswirken der unterschiedlichen Treebank arten für Parser!! 'Die' Treebank gibt es nicht - Diese These be - oder widerlegen!
Außerdem weist dieses neue Konzept auch zwei Nachteile auf.
Das erste Problem der \textit{PCFG} ergibt sich aus der Kontextfreiheit. Die Wahrscheinlichkeit einer Produktion ist immer gleich, egal an welcher Stelle im Satz sie auftritt. In der tatsächlichen natürlichen Sprache ist das aber im Allgemeinen nicht der Fall. Beispielweise kann die gewählte Produktion einer Nominalphrase abhängig davon sein, ob die Phrase Objekt oder Subjekt des Satzes ist. Da diese Information ohne Weiteres nicht in der Grammatik berücksichtigt werden kann, muss der Mittelwert aus beiden Fällen gebildet werden. Dies führt dazu, dass entweder im Falle des Objekts oder des Subjekts häufig die falsche Regel angewendet wird. \\
Als zweite Schwachstelle ergibt sich, dass die einzelnen Wörter eine zu kleine Rolle spielen. Als Beispiel hierfür dient die bereits erklärte Anhangs-Mehrdeutigkeit aus Kapitel \ref{sec:nlp:syn-parsen}. Wiederum wird eine Präpositionalphrase betrachtet, welche entweder an eine Verbal- oder eine Nominalphrase angebunden wird. An welche von beiden hängt allein von der \textit{Treebank} ab. Dort kommt entweder die Produktion \[ VP  \to  \alpha \;\;  NP \;\; PP \] oder die Kombination aus \[ VP  \to  \alpha \;\; NP \] und \[ NP  \to  NP \;\; PP \] öfter vor. \( \alpha \) steht für eines der hier möglichen Verb-Nichtterminale, welches aber in beide Fällen immer das selbe ist und deswegen keine Rolle spielt. Wesentlich bessere Resultate sind hier erzielbar, wenn man das Verb aus \textit{VP}, das Nomen aus \textit{NP} und die Präposition aus \textit{PP} in die Entscheidungsfindung mit einbezieht. Es kann aus der \textit{Treebank} die Information gewonnen werden, ob die gegebene Präposition sich öfter auf das Nomen oder das Verb bezieht. Angenommen es gibt eine Präposition, die ausschließlich mit Verben in Verbindung steht. In der \textit{Treebank} sind es nun aber die Nominalphrasen, an welche öfter Präpositionalphrasen gebunden werden. Dann wird die eben angenommene Präposition mit einer \textit{PCFG}, welche mit der \textit{Treebank} errechnet wurde, immer falsch zugeordnet. \\ Diese Schwäche der \textit{PCFG} macht sich ebenso bei Koordinations-Mehrdeutigkeit bemerkbar. Beim Verbinden von Phrasen durch Konjunktionen kann wieder die konkrete Konjunktion und die Beziehung zu den entsprechenden Wörtern der zu verbindenden Phrasen betrachtet werden. Betrachtet man nochmal das Beispiel aus Abschnitt \ref{sec:nlp:syn-parsen}: \textit{old men and women}. Hier könnte eine \textit{Treebank} die Information liefern, dass die Wörter \textit{men} und \textit{women} per \textit{and} öfter direkt miteinander verbunden werden, als dass nur eines von beiden das Adjektiv \textit{old} zugeordnet bekommt. \\
Eine mögliche Verbesserung der \textit{PCFG} ergibt sich durch das Erweitern der Nichtterminale um die Information wessen Kind es ist. Es wird also ein \textit{NP} als \textit{NP\^{}S} geschrieben, wenn \textit{S} der Elternknoten ist oder als \textit{NP\^{}VP}, falls es \textit{VP} ist. Hierdurch ergeben sich zwei neue Regeln in der Grammatik und damit auch zwei neue Wahrscheinlichkeiten. Mit diesem Konzept kann dem Nichtterminal, obwohl die Kontextfreiheit im grammatikalischen Sinne nicht verletzt wird, ein Kontext gegeben werden. Allerdings wird, wenn jedes Nichtterminal für jeden möglichen Elternknoten eine neue Produktion erhält, die Grammatik enorm aufgeblasen. Nebeneffekt dieser neuen Größe ist, dass bei gleich bleibender \textit{Treebank} für jede Regel weniger Trainingsdaten zur Verfügung stehen und damit die erhaltenen Wahrscheinlichkeitswerte ungenauer sind. Für einen gegebenen Korpus muss also ein Split-and-Merge Algorithmus ausgeführt werden, der errechnet wie weit eine Aufteilung der Nichtterminale sinnvoll ist.  

\subsection{Probabilistische Lexikalisierte Kontextfreie Grammatiken}
\label{sec:nlp:stat-parsen:plcfg}

Ein weiterer Ansatz, um bessere Parserergebnisse zu erhalten, ist das Lexikalisieren der Grammatik. Hierfür muss der Begriff des Kopfes, im Englischen \textit{Head}, eingeführt werden. Die Idee ist, dass jede syntaktische Einheit einen Kopf, in Form eines Wortes aus dieser Einheit, besitzt. Es wird das Wort genommen, welches ``im Satz am grammatikalisch wichtigsten ist'' \cite[S. 443]{nlpGrundlagen}. %TODO Def. einbauen und Seite 443 zitieren.
Da jedes Nichtterminal einer syntaktischen Einheit oder einem POS-Tag entspricht, kann jedem Nichtterminal und jeder Produktion der Grammatik ein Kopf zugewiesen werden. Für das Finden des richtiges Wortes gibt es verschiedene Schritte. Begonnen wird, indem jedes POS-Nichtterminal sein entsprechendes Kind als \textit{Head} wählt. Dieses Wort wird dann im Baum nach oben weitergegeben. Für jede syntaktische Einheit gibt es Regeln, anhand derer einer der Köpfe der Kinder ausgewählt und als eigener eingesetzt wird. Ein Beispiel ist der lexikalisierte Baum für den Satz \textit{workers dumped sacks into a bin}, dargestellt in Abbildung \ref{fig:lex-tree-dumped-sacks}. %TODO Ref Buch Seite 445
\\
\begin{figure}
\qtreecentertrue\Tree [.S(dumped) [.NP(workers) [.NNS(workers) workers ] ] [.VP(dumped) [.VDB(dumped) dumped ] [.NP(sacks) [.NNS(sacks) sacks ] ] [.PP(into) [.IN(into) into ] [.NP(bin) [.DT(a) a ] [.NN(bin) bin ] ] ] ] ]
\label{fig:lex-tree-dumped-sacks}
\caption{Lexikalisierter Baum, entnommen aus} %TODO zitieren
\end{figure}
Zusätzlich zum Kopfwort kann auch noch dessen POS-Tag abgespeichert werden. Somit wird beispielsweise \textit{S(dumped)} zu \textit{S(dumped, VBD)} und \textit{NNS(workers)} zu \textit{NNS(workers, NNS)}. Die Wahrscheinlichkeit der Regel 
\begin{equation}\label{eqn:lexikal-dumped-sacks}
VP(dumped, VBD)  \to  VBD(dumped, VBD) \;\;  NP(sacks, NNS) \;\; PP(into, IN) 
\end{equation} %TODO alle Formeln so bennen
ergibt sich wieder aus dem Inhalt der \textit{Treebank}, nämlich durch die Formel
\begin{equation}
\frac{Anzahl(VP(dumped, VBD)  \to  VBD(dumped, VBD) \;  NP(sacks, NNS) \; PP(into, IN))}{Anzahl(VP(dumped, VBD))} 
\end{equation}
Dieser Wert ist null, falls der Korpus keinen Satz mit \textit{dumped} \textit{sacks} \textit{into} enthält. Existiert kein Satz in welchem sich \textit{VP(dumped, VBD)} finden lässt, so ist dieser Wert nicht definiert. \\
Aufgrund dieses Problems bedarf es anderer Berechnungsvorschriften, wie zum Beispiel \textit{Collins Model 1}, vorgestellt in \cite{collinsModel}. %TODO Collins Model 1 Paper zitieren
Es wird jede Produktion folgendermaßen betrachtet: \textit{H} ist der Kopf, \( L_i \) sind die Nichtterminale links und \( R_i \) die rechts davon. Alle Nichtterminale bleiben weiterhin lexikalisiert. Das Kopfwort wird mit \textit{h} bezeichnet. Damit ergibt sich die Form
\[ A \to L_n...L_1 H R_1...R_m \]
Zusätzlich wird an der Stelle \( L_{n+1} \) und \( R_{m+1} \) das Nichtterminal \textit{STOP} eingefügt um anzuzeigen, dass hiernach die Regel zu Ende ist. In drei Schritten wird die Wahrscheinlichkeit der Produktion errechnet:
\begin{enumerate}
\item Es wird der Kopf mit der Wahrscheinlichkeit \( P_H(H | A, h) \) generiert.
\item Alle Elemente rechts vom Kopf werden mit \( \displaystyle\prod_{i = 1}^{m+1} P_R(R_i | A, h, H) \), also einschließlich dem \textit{STOP}, generiert.
\item Alle Elemente links von \textit{H} werden mit \( \displaystyle\prod_{i = 1}^{n+1} P_L(L_i | A, h, H) \) generiert.
\end{enumerate}
Für die Regel \ref{eqn:lexikal-dumped-sacks} errechnet sich die Wahrscheinlichkeit über
\begin{align}
P & = P_H(VBD|VP, dumped) \nonumber \\ & \times P_R(NP(sacks, NNS)|VP, dumped, VBD) \nonumber \\ & \times P_R(PP(into, IN)|VP, dumped, VBD) \nonumber \\ & \times P_R(STOP|VP, dumped, VBD) \nonumber \\ & \times P_L(STOP|VP, dumped, VBD)
\end{align}
Im Gegensatz zu vorher muss also nicht die Kombination aus \textit{NP(sacks, NNS)}, \textit{PP(into, IN)} und \textit{VBD(dumped, VBD)} vorhanden sein. Es genügt, wenn jedes einzeln, als Kind von \textit{VP(dumped, VBD)} auf der entsprechenden Seite des \textit{Heads} in der \textit{Treebank} zu finden ist. %TODO eventuell erklären warum man das so machen kann: Markov, MLE, ... %TODO Distanz, Model 2 und 3 erklären?
Darüber hinaus wird das Modell noch um die Information der Distanz erweitert. Es wird also zusätzlich berücksichtigt, wie weit ein Nichtterminal vom Kopf der Regel entfernt ist. 
Der Vollständigkeit halber ist außerdem zu erwähnen, dass es neben \textit{Model 1} noch zwei weitere Modelle gibt. Diese Modelle bilden die Grundlage für den \textit{Collins Parser}. %TODO zu finden unter...