% !TEX root = ../thesis-example.tex
%
\chapter{Natural Language Processing}
\label{sec:nlp}

\section{Grundlagen}
\label{sec:nlp:grundlagen}

Betrachtet man einen Satz in einer natürlichen Sprache, die im Rahmen dieser Arbeit auf Englisch festgelegt ist, so kann ein Computer dessen Inhalt nicht ohne Weiteres herauslesen. Hierfür bedarf es verschiedener Hilfsstrukturen und zusätzlicher Informationen. Als ersten Schritt bietet es sich an, den einzelnen Wörtern eines Satzes ihre Wortart zuzuordnen. Mit Wortart, alternativ auch Wortklasse oder zu englisch part-of-speech (POS), ist gemeint, wie ein Wort im Satz auftritt. Beispiele für Wortarten sind Nomen, Verb und Adjektiv. In dieser Arbeit wird das Tagset, also die Menge an Wortarten, aus der Penn Treebank verwendet. %TODO hier zitieren und evtl Tagset als Tabelle anzeigen
%TODO Außerdem Quelle für POS Informationen suchen oder Buch nehmen 
Der Satz
\begin{quote}
My dog also likes eating sausage.
\end{quote}
würde mit diesem Tagset also folgendermaßen annotiert werden:
\begin{quote}
My/PRP\$ dog/NN also/RB likes/VBZ eating/VBG sausage/NN ./.
\end{quote}
Wie ein Satz maschinell mit POS-Tags versehen werden kann wird in dieser Arbeit nicht weiter behandelt. 
Über diese zusätzliche Notation hinaus kann man erkennen, dass sich in der englischen Sprache oftmals mehrere Wörter als Gruppe oder als eine Komponente innerhalb des Satzes verhalten. So eine Gruppe wäre zum Beispiel die Nominalphrase \textit{My Dog} oder die Verbalphrase \textit{likes eating sausage}. Auch hier wird wieder die Annotation der Penn Treebank verwendet. %TODO Siehe Abbildung mit Phrase Tags, welche version wird verwendet? 2 oder 3 oder ...?
Der Satz, welcher sich aus der zusätzlichen Annotation ergibt, lautet:
\begin{lstlisting}
(S
  (NP (PRP$ My) (NN dog))
  (ADVP (RB also))
  (VP (VBZ likes)
    (S
      (VP (VBG eating)
        (NP (NN sausage)))))
  (. .))
\end{lstlisting}
%TODO hier noch Bäume reinbringen und Baum zeichnen!
Alternativ zur geklammerten Lösung kann man dieses Ergebnis auch als Syntaxbaum zeichnen:\\

\qtreecentertrue\Tree [.S [.NP [.PRP My ] [.NN dog ] ] [.ADVP [.RB also ] ] [.VP [.VBZ likes ] [.S [.VP [.VBG eating ] [.NP [.NN sausage ] ] ] ] ] [.. . ] ]\\\\
Wie man am Beispiel erkennen kann, sind auch diverse Verschachtelungen dieser Komponenten möglich. Um diese Anordnungsstruktur innerhalb einer Sprache zu beschreiben, bieten sich kontextfreie Grammatiken an. Auf der linken Seite der Regeln befindet sich also ein Nichtterminalsymbol und auf der rechten Seite können sich beliebig viele Terminale und Nichtterminale befinden. Für die Verarbeitung unseres Beispielsatzes wurden unter anderem folgende Regeln verwendet: %Exkurs CFG?
\begin{lstlisting}
S -> NP ADVP VP .
NP -> PRP$ NN
PRP$ -> My
NN -> dog
\end{lstlisting}
%TODO listing durch Formel ersetzen
Das Startsymbol für Grammatiken einer natürlichen Sprache ist oftmals \textit{S}, was als das englische Wort \textit{sentence} aufgefasst werden kann. %TODO zitat aus kapitel 12.2
Anhand dem vollständigen Regelsatz der Grammatik einer Sprache kann man theoretisch jeden grammatikalisch korrekten Satz dieser Sprache annotieren. Es gibt für diverse natürliche Sprache Sammlungen von annotierten Sätzen, diese werden Treebank oder Korpus genannt. Ein bekannter Korpus der englischen Sprache ist die Penn Treebank, aus welcher auch die hier verwendete Annotation stammt. Dieser Korpus wird vom Linguistic Data Consortium, mit Sitz in der Univertität von Pennsylvania, herausgegeben. %TODO https://www.ldc.upenn.edu/about#
%Er umfasst über 4,5 Millionen Wörter 
%TODO Recherche über PTB betreiben - aktuelle Infos bekommen - Seite 442 im Buch nochmal genauer anschauen
Im Rahmen des Penn Treebank Projekts wurden von 1989 bis 1992 über 4,5 Millionen Wörter der Treebank hinzugefügt. Die Texte hierfür stammen zu einem Großteil aus dem Wall Street Journal. Der Ursprung der Sätze in einer Treebank kann eine Rolle spielen, da Parser mit Hilfe von Treebanks trainiert werden. Dazu mehr in Kapitel ... %TODO verweis auf kapitel parser trainieren und zitat von paper ptb_info

\section{Syntaktisches Parsen}
\label{sec:nlp:syn-parsen}

Syntaktisches Parsen wird als die "Aufgabe des Erkennens eines Satzes und des Zuweisens einer syntaktischen Struktur" definiert. %TODO zitat s.461
%TODO Def: Paren als richtige Def, danach gleich Def: Parser = Programm oder sowas, welches Parsen durchführt
Zum Einstieg in das Kapitel wird das Parsen als Suche betrachtet. Das Suchproblem besteht darin, aus allen möglichen Bäumen, welche sich mit der Grammatik generieren lassen, den korrekten Baum zur Eingabe zu finden. Der Suchraum wird also von der Grammatik festgelegt. Ein Baum ist korrekt, wenn \textit{S} die Wurzel ist und exakt die Eingabe abgedeckt wird. Anhand dieser zwei Merkmale kann die Suche gestaltet werden. Somit ergeben sich als grundlegende Ansätze die Top-Down und die Bottom-Up Suche. Beim Top-Down Verfahren wird mit dem Startsymbol \textit{S} begonnen und dieses mit den Regeln der Grammatik Schritt für Schritt erweitert. Bäume deren Blätter nicht auf die Eingabe passen werden abgelehnt. Die Bottom-Up Suche beginnt, dem Namen entsprechend, am anderen Ende des Baumes. Im ersten Schritt gibt es nur die Eingabeworte als Blätter. Es wird mit den rechten Seiten der Grammatikregeln der Baum nach oben gebaut. Hier kann also ein Baum ausgeschlossen werden, wenn seine obersten Knoten in keiner Kombination auf keiner rechten Seite einer Produktion vorkommen. %Der Vorteil des einen Ansatz entspricht in etwa dem Nachteil des anderen. 
So werden mit der Top-Down Suche keine Bäume gebaut die niemals das Start Symbol als Wurzel haben, dafür wird aber die Eingabe im Allgemeinen nicht abgedeckt. Beim Bottom-Up Ansatz verhält es sich genau andersherum. 
Das Hauptproblem, welches sich beim Finden des korrekten Baumes ergibt, ist die Mehrdeutigkeit. Zum einen können Wörter mehrdeutig sein, wie etwa das englische Wort \textit{book}, welches sowohl Verb als auch Nomen ist. Zum anderen, und für diesen Kontext relevanter, gibt es die strukturelle Mehrdeutigkeit. Ein Beispielsatz hierfür ist: 
\begin{quote}
I shot an elephant in my pajamas.
\end{quote}
In dieser Satzstruktur kann sich \textit{in my pajamas} sowohl auf den Erzähler, als auch auf den Elefanten beziehen und gibt es mehr als einen korrekten Baum. % Grammatikalisch sind also mehrere Bäume korrekt, inhaltlich aber nur einer. %TODO Hier besser ausdrücken
Diese Art der strukturellen Mehrdeutigkeit ist die Anhangs-Mehrdeutigkeit. Eine Komponente des Satzes, in diesem Fall die Präpostionalphrase, kann an mehreren Stellen angehängt werden. Kombiniert man die Präpositional- an die Verbalphrase hat der Satz die Bedeutung, dass das Schießen im Pyjama stattgefunden hat. Bindet man diese an das Nomen Elefant wird ausgesagt, dass dieser sich im Pyjama befindet. Grammatikalische Korrektheit ist in beiden Fällen gegeben. Eine andere Art ist die Koordinations-Mehrdeutigkeit, welche in Verbindung mit Konjunktionen und Satzverbindungen auftritt. Beispielsweise kann der Satzausschnitt \textit{old men and women} unterschiedlich interpretiert werden. \textit{Old} kann sich auf \textit{men and women} oder nur auf \textit{men} beziehen. \\
Die Anzahl an unterschiedlichen und dennoch korrekten Bäumen für einen Satz kann also oft groß sein. %TODO kann sogar exponentiell wachsen, Seite 467
Von diesen Bäumen beschreibt aber nur einer den Inhalt des Satzes so, wie er vom Autor gemeint ist. Ein Parser braucht als weitere Kriterien um sich zwischen den verschiedenen Möglichkeiten entscheiden zu können. Hierzu mehr in Kapitel \ref{sec:nlp:stat-parsen}. Ohne zusätzliche Informationen kann der Parser nur alle möglichen Bäume erstellen. Um das effizient zu bewerkstelligen bietet sich dynamisches Programmieren an. 

\subsection{Dynamische Programmierung}
\label{sec:nlp:syn-parsen:dyn-progr}

Dynamisches Programmieren ist hier sinnvoll, da beim Erstellen des Baumes im Allgemeinen Mehrfacharbeit anfällt. Baut der Parser zum Beispiel die Bäume von oben nach unten und versucht dabei die Eingabe von links nach rechts abzudecken, dann findet er in der Regel einen Baum der einen Teil des Satzes abdeckt. Da noch nicht die gesamte Eingabe enthalten ist, handelt es sich nicht um einen korrekten Baum. Dennoch kann dieser Baum als Teilbaum in tatsächlichen Lösung enthalten sein. Der Parser müsste ihn aber ohne dynamisches Programmieren immer wieder neu finden. Als Algorithmen zum Parsen haben sich das Chart Parsing, der Earley Algorithmus und der Cocke-Kasami-Younger Algorithmus etabliert.
%TODO komplettes Kapitel noch eintragen

\section{Statistisches Parsen}
\label{sec:nlp:stat-parsen}

In diesem Kapitel wird vorgestellt, wie ein Parser sich zwischen mehreren korrekten Bäumen zu einer Eingabe entscheiden kann. Hierfür benötigt er für jeden errechneten Baum zusätzlich die Wahrscheinlichkeit von eben diesem. Das heißt, jede Lösung ist mit einer Wahrscheinlichkeit versehen und es kann diejenige, deren Wert am höchsten ist als Lösung gewählt werden. %TODO letzten Satz evtl raus nehmen und ausführlich bei PCFG erklären

\subsection{Probabilistische Kontextfreie Grammatiken}
\label{sec:nlp:stat-parsen:pcfg}

Die Anforderung, einem Baum eine Wahrscheinlichkeit zuzuweisen, lässt sich mit dem Konzept der probabilistischen kontextfreien Grammatiken (abgekürzt: PCFG) umsetzen. Abgesehen von zwei Erweiterungen haben die Produktionen einer PCFG die gleiche Form wie die der ursprünglichen kontextfreien Grammatik. Erstens wird jeder Regel eine Wahrscheinlichkeit \textit{p}, mit \( 0 \leq p \leq 1 \), hinzugefügt.
\[ A \to \beta  [p] \]
Diese gibt an, mit welcher Wahrscheinlichkeit die rechte Seite und der Vorbedingen der linken Seite auftritt.
\[ p := P(A \to \beta | A) \] 
Es handelt sich also um eine bedingte Wahrscheinlichkeit.\\ 
Zweitens muss für jedes Nichtterminal die Summe der Wahrscheinlichkeiten aller seiner Produktionen 1 ergeben.
\[ \sum_{\beta} P(A \to \beta) = 1 \] 
Die Wahrscheinlichkeit für einen Baum errechnet sich dann durch das Produkt der Wahrscheinlichkeiten aller verwendeten Regeln. Eine Grammatik heißt konsistent, falls die Summe der Wahrscheinlichkeiten aller möglichen Sätze 1 ergibt. Inkosistenz tritt auf, falls eine Regel der Form 
\[ A \to A \]
gibt.\\
Mit dieser neuen Art von Grammatik ergeben sich auch neue Algorithmen zum Berechnen der Bäume. Sowohl der CKY als auch der Earley Algorithmus sind um den Faktor der Wahrscheinlichkeit erweiterbar, wobei der CKY Algorithmus mehr Verwendung findet.\\ %TODO diesen Satz heraus nehmen oder so lassen oder CKY erklären, dann aber auch normalen kurz erklären...
Einen Nutzen kann man aus der zugefügten Wahrscheinlichkeit nur dann ziehen, wenn ihr numerischer Wert Sinn ergibt. Um diesen Wert für jede Regel zu berechnen gibt es zwei Möglichkeiten. Zum einen kann dieser aus einer vollständig annotierten Treebank errechnet werden. Hierfür wird jedes Auftreten einer Regel und des entsprechenden Nichtterminals gezählt und dividiert. 
Falls man keine solche Treebank zur Verfügung hat, gibt es noch eine zweite Möglichkeit die Werte der PCFG festzulegen. Hierzu arbeitet der Parser einen unannotierter Textkorpus durch und versieht die Sätze mit Tags. Zu Beginn haben alle Produktionen eines Nichtterminals die selbe Wahrscheinlichkeit. Der Korpus wird iterativ durchlaufen und nach jedem Durchgang werden die Wahrscheinlichkeiten der Regeln angepasst. Das Anpassen passiert wie in der ersten vorgestellten Möglichkeit, da zu diesem Zeitpunkt eine annotierte Treebank vorhanden ist. Das Verfahren endet, wenn die Wahrscheinlichkeiten konvergieren. \\
Die resultierenden Werte in der Grammatik hängen also davon ab, mit welchem Korpus sie berechnet wurden. Hierbei spielen Größe und Textart eine große Rolle. Um beim Parsen eines Textes möglichst gute Ergebnisse zu erhalten, sollte der Parser eine PCFG verwenden, welche mit einem Text des selben Genres erstellt wurde. So kann man beispielsweise mit einer Treebank aus technischen Handbüchern, unabhängig von ihrer Größe, beim Parsen eines privaten Briefes keine guten Ergebnisse erwarten, da sich die Sprache zu sehr unterscheidet.\\ %TODO Paper suchen über auswirken der unterschiedlichen Treebank arten für Parser!! 'Die' Treebank gibt es nicht - Diese These be - oder widerlegen!
Außerdem weißt dieses neue Konzept auch zwei Nachteile auf.
Das erste Problem der PCFG ergibt sich aus der Kontextfreiheit. Die Wahrscheinlichkeit einer Produktion ist immer gleich, egal an welcher Stelle im Satz sie auftritt. In der tatsächlichen natürlichen Sprache ist das aber im Allgemeinen nicht der Fall. Beispielweise kann die gewählte Produktion einer Nominalphrase abhängig davon sein ob die Phrase Objekt oder Subjekt des Satzes ist. Da diese Information ohne weiteres nicht in der Grammatik nicht berücksichtigt werden kann muss der Mittelwert aus beiden Fällen gebildet werden. Dies führt dazu, dass entweder im Falle des Objekts oder des Subjekts häufig die falsche Regel angewendet wird. 
Als zweite Schwachstelle ergibt sich, dass die einzelnen Wörter eine zu kleine Rolle spielen. Als Beispiel hierfür dient die bereits erklärte Anhangs-Mehrdeutigkeit aus Kapitel \ref{sec:nlp:syn-parsen}. Wiederum wird eine Präpositionalphrase betrachtet, welche entweder an eine Verbal- oder eine Nominalphrase angebunden wird. An welche von beiden hängt wieder allein von der Treebank ab. Dort kommt entweder die Produktion \[ VP  \to  \alpha \;\;  NP \;\; PP \] oder die Kombination aus \[ VP  \to  \alpha \;\; NP \] und \[ NP  \to  NP \;\; PP \] öfter vor.\( \alpha \) steht für eines der hier möglichen Verb-Nichtterminale, welches aber in beide Fällen immer das selbe ist und deswegen keine Rolle spielt. Wesentlich bessere Resultate sind hier erzielbar, wenn man das Verb aus \textit{VP}, das Nomen aus \textit{NP} und die Präposition aus \textit{PP} in die Entscheidungsfindung mit einbezieht. Es kann aus der Treebank die Information gewonnen werden, ob die gegebene Präposition sich öfter auf das Nomen oder das Verb bezieht. Angenommen es gibt eine Präposition die ausschließlich mit Verben in Verbindung steht. In der Treebank sind es nun aber die Nominalphrasen, an welche öfter Präpositionalphrasen gebunden werden, dann wird die eben angenommene Präposition mit einer PCFG immer falsch zugeordnet. Diese Schwäche der PCFG macht sich ebenso bei Koordinations-Mehrdeutigkeit bemerkbar. Beim Verbinden von Phrasen durch Konjunktionen kann wieder die konkrete Konjunktion und die Beziehung zu den entsprechenden Wörtern der zu verbindenden Phrasen betrachtet werden. Betrachtet man nochmal das Beispiel aus \ref{sec:nlp:syn-parsen}: \textit{old men and women}. Hier könnte eine Treebank die Information liefern, dass die Wörter \textit{men} und \textit{women} per \textit{and} öfter direkt miteinander verbunden werden als dass nur eines von beiden das Adjektiv \textit{old} zugeordnet bekommt. \\

\begin{tabular}{ | l l l | l l l |}
	\hline
	\multicolumn{6}{|c|}{Penn Treebank Part-of-Speech Tags} \\
	\hline
	CC & Koordinierende Konjunktion & \textit{and} & TO & to & what \textit{to} do \\
	CD & Kardinalzahl & \textit{third} & UH & Ausruf & \textit{oops} \\
	DT & Artikel & \textit{the} & VB & Verb, Grundform & \textit{be} \\
	Ex & Existentielles \textbf{there} & \textit{there} is & VBD & Verb, Vergangenheitsform & \textit{was} \\
	FW & Fremdword & \textit{les} & VBG & Verb, Gerund \textbackslash Partizip Präsens & \textit{being} \\
	IN & Präposition, unterordnende Konjunktion & \textit{in} & VBN & Verb, Partizip Perfekt & \textit{been} \\
	JJ & Adjektiv & \textit{green} & VBP & Verb, Präsens, nicht 3.Person Singular & \textit{am} \\
	JJR & Adjektiv, Komparativ & \textit{greener} & VBZ & Verb, Präsens, 3.Person Singular & \textit{is} \\
	JJS & Adjektiv, Superlativ & \textit{greenest} & WDT & \textit{wh}-Artikel & \textit{which} \\
	LS & Listenelement Markierung & \textit{1)} & WP & \textit{wh}-Pronomen & \textit{who} \\
	MD & Modal & \textit{could} & WP\$ & \textit{wh}-Possesivpronomen & \textit{whose} \\
	NN & Nomen, singular oder Masse & \textit{table} & WRB & \textit{wh}-Adverb & \textit{be} \\
	NNS & Nomen, plural & \textit{tables} & \# & \# & \textit{\#} \\
	NNP & Eingenname, singular & \textit{Germany} & \$ & \$ & \textit{\$} \\
	NNPS & Eigenname, plural & \textit{Vikings} & . & . & \textit{.} \\
	PDT & Predeterminer & \textit{both} his children & , & , & \textit{,} \\
	POS & possesive Endung & \textit{'s} & : & Colon, Semicolon & \textit{:} \\
	PRP & Personalpronomen & \textit{me} & ( & ( & \textit{(} \\
	PRP\$ & Possesivpronomen & \textit{my} & ) & ) & \textit{)} \\
	RB & Adverb & \textit{extremely} & `` & Öffnendes Anführungszeichen & \textit{``} \\
	RBR & Adverb, Komparativ & \textit{better} & " & Schließendes Anführungszeichen & \textit{"} \\
	RBS & Adverb, Superlativ & \textit{best} &  & Colon, Semicolon & \textit{:} \\
	RP & Partikel & \textit{about} &  & Colon, Semicolon & \textit{:} \\
	SYM & Symbol & \textit{\%} &  & Colon, Semicolon & \textit{:} \\
	
	
	
	
	
	\hline
\end{tabular}